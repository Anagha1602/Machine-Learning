{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfux7jAv3ymG"
      },
      "outputs": [],
      "source": [
        " import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the .txt file\n",
        "with open(\"/content/textfile.txt\") as file:\n",
        "    lyrics_text = file.read()"
      ],
      "metadata": {
        "id": "c_sh72_U4A3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the lyrics\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([lyrics_text])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "srwC4wV04Hdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences in smaller batches\n",
        "batch_size = 1000\n",
        "input_sequences = []\n",
        "token_list = tokenizer.texts_to_sequences([lyrics_text])[0]\n",
        "for i in range(0, len(token_list), batch_size):\n",
        "    n_gram_sequence = token_list[i:i+batch_size]\n",
        "    input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "CloptHRK4K4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences\n",
        "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding=\"pre\")"
      ],
      "metadata": {
        "id": "-_DLxZWu4WJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create predictors and labels\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
      ],
      "metadata": {
        "id": "vKWMeWEn4Zbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(total_words, 100, input_length=max_sequence_length-1))\n",
        "model_lstm.add(LSTM(150))\n",
        "model_lstm.add(Dense(total_words, activation=\"softmax\"))\n",
        "model_lstm.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "3YvsUzy_4cpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train LSTM model\n",
        "with tf.device('/device:GPU:0'):\n",
        "    model_lstm.fit(X, y, epochs=5, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "qlMkGdVL4gHO",
        "outputId": "8ca007ea-9884-48ea-eec5-843b08b7368f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-be462eb4f378>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train LSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Bidirectional LSTM model\n",
        "model_bidirectional = Sequential()\n",
        "model_bidirectional.add(Embedding(total_words, 100, input_length=max_sequence_length-1))\n",
        "model_bidirectional.add(Bidirectional(LSTM(150)))\n",
        "model_bidirectional.add(Dense(total_words, activation=\"softmax\"))\n",
        "model_bidirectional.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "Xy8dEVcY4jMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Bidirectional LSTM model\n",
        "with tf.device('/device:GPU:0'):\n",
        "    model_bidirectional.fit(X, y, epochs=3, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTZOBGyn4mOo",
        "outputId": "66fac7f6-ae80-403d-ac2b-1cc57fa7b9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1/1 [==============================] - 4s 4s/step - loss: 5.1933 - accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 5.1382 - accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.0776 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate text using LSTM\n",
        "seed_text = \"Anagha is waiting\"\n",
        "next_words = 5\n",
        "def generate_text(model, seed_text, next_words):\n",
        "    generated_text = seed_text\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding=\"pre\")\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "        predicted_index = np.random.choice(len(predicted_probs[0]), p=predicted_probs[0])\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "        generated_text += \" \" + output_word\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "3wZpmFno4pfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using LSTM\n",
        "lstm_generated_text = generate_text(model_lstm, seed_text, next_words)\n",
        "print(f\"LSTM generated text: {lstm_generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQy3qrWM4swb",
        "outputId": "eb5da3a9-d7d7-474e-cd4b-f69f83948b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM generated text: Anagha is waiting will debt public short spurts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using Bidirectional LSTM\n",
        "bidirectional_generated_text = generate_text(model_bidirectional, seed_text, next_words)\n",
        "print(f\"Bi-LSTM generated text: {bidirectional_generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvFkkQof4vf-",
        "outputId": "93027232-901b-4a10-8266-37786689611b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bi-LSTM generated text: Anagha is waiting falls shoppers the short us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TgpgE6Zf4zcy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}